name: daily-exchange-rates

on:
  schedule:
    - cron: '0 16 * * *' # 00:00 Australia/Perth -> 16:00 UTC previous day
  workflow_dispatch: {}

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f fetch_historical_exchange_rate/requirements.txt ]; then
            pip install -r fetch_historical_exchange_rate/requirements.txt
          else
            pip install requests pandas google-cloud-bigquery pyarrow python-dotenv
          fi

      - name: Prepare .env (from Secrets)
        run: |
          echo "EXCHANGE_API_KEY=${{ secrets.EXCHANGE_API_KEY }}" > fetch_historical_exchange_rate/.env

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Ensure data & logs dir exists
        run: mkdir -p fetch_historical_exchange_rate/data/logs

      - name: Run fetch step (log output)
        working-directory: fetch_historical_exchange_rate
        run: |
          mkdir -p data/logs
          python3 scripts/fetch_historical_rate.py >> data/logs/fetch_historical_rate.log 2>&1
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_DATASET: ${{ secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}

      - name: Run dim_currency load (log output)
        working-directory: fetch_historical_exchange_rate
        run: |
          mkdir -p data/logs
          python3 scripts/create_dim_currency.py >> data/logs/create_dim_currency.log 2>&1
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_DATASET: ${{ secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}

      - name: Run ETL step (log output)
        working-directory: fetch_historical_exchange_rate
        run: |
          mkdir -p data/logs
          python3 scripts/extract_transform.py >> data/logs/extract_transform.log 2>&1
        env:
          BQ_PROJECT: ${{ secrets.BQ_PROJECT }}
          BQ_DATASET: ${{ secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}

      - name: Ensure at least one log file exists (placeholder)
        run: |
          touch fetch_historical_exchange_rate/data/logs/.gitkeep

      - name: Ensure artifact paths exist
        run: |
          ls -la fetch_historical_exchange_rate || true
          ls -la fetch_historical_exchange_rate/data || true

      - name: Upload pipeline logs
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs
          path: fetch_historical_exchange_rate/data/logs/**
