name: daily-exchange-rates

on:
  schedule:
    - cron: '0 16 * * *' # 00:00 Australia/Perth -> 16:00 UTC previous day
  workflow_dispatch: {}

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f fetch_historical_exchange_rate/requirements.txt ]; then
            pip install -r fetch_historical_exchange_rate/requirements.txt
          else
            pip install requests pandas sqlalchemy python-dotenv
          fi

      - name: Prepare .env (from Secrets)
        run: |
          echo "EXCHANGE_API_KEY=${{ secrets.EXCHANGE_API_KEY }}" > fetch_historical_exchange_rate/.env

      - name: Ensure data & logs dir exists
        run: mkdir -p fetch_historical_exchange_rate/data/logs

      - name: Run fetch step (log output)
        working-directory: fetch_historical_exchange_rate
        run: |
          mkdir -p data/logs
          python3 scripts/fetch_historical_rate.py >> data/logs/fetch_historical_rate.log 2>&1

      - name: Run ETL step (log output)
        working-directory: fetch_historical_exchange_rate
        run: |
          mkdir -p data/logs
          python3 scripts/extract_transform.py >> data/logs/extract_transform.log 2>&1

      - name: Ensure at least one log file exists (placeholder)
        run: |
          touch fetch_historical_exchange_rate/data/logs/.gitkeep

      - name: Ensure artifact paths exist
        run: |
          ls -la fetch_historical_exchange_rate || true
          ls -la fetch_historical_exchange_rate/data || true

      - name: Upload exchange_rates.db artifact
        uses: actions/upload-artifact@v4
        with:
          name: exchange_rates_db
          path: fetch_historical_exchange_rate/exchange_rates.db

      - name: Upload pipeline logs
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs
          path: fetch_historical_exchange_rate/data/logs/**

      # --- New: upload DB to Google Cloud Storage ---
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Upload exchange_rates.db to GCS
        env:
          BUCKET: ${{ secrets.GCS_BUCKET }}
          PREFIX: ${{ secrets.GCS_PATH || 'exchange_rates' }}
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          SRC="fetch_historical_exchange_rate/exchange_rates.db"
          DEST="gs://$BUCKET/${PREFIX}/exchange_rates-$TIMESTAMP.db"
          echo "Uploading $SRC -> $DEST"
          gsutil cp "$SRC" "$DEST"